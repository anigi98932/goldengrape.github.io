<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GoldenGrape's Blog (文章分类：tutorial)</title><link>https://goldengrape.github.io/</link><description></description><atom:link href="https://goldengrape.github.io/categories/cat_tutorial.xml" rel="self" type="application/rss+xml"></atom:link><language>zh_cn</language><copyright>Contents © 2018 &lt;a href="mailto:"&gt;Golden Grape&lt;/a&gt; </copyright><lastBuildDate>Fri, 05 Jan 2018 09:33:02 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>机器学习的战略(6)--end to end</title><link>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-6-end-to-end/</link><dc:creator>Golden Grape</dc:creator><description>&lt;div&gt;&lt;h2&gt;端到端学习 end-to-end Learning&lt;/h2&gt;
&lt;p&gt;这个也是吓人.  读书人, 相煎何太急.&lt;/p&gt;
&lt;p&gt;end-to-end的一端是原始数据, 另一端是想要的结果. 比如也许从一张眼底照片+FFA直接给出眼底激光应该打哪里, 或者说中文直接给出翻译好的英文字幕. 从一端到另一端中间本来是有很多研究过程在里面的,
&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;中文语音-&amp;gt;音频信号增强降噪-&amp;gt;音节提取-&amp;gt;转成文字-&amp;gt;翻译-&amp;gt;英文字幕,&lt;/li&gt;
&lt;li&gt;眼底照片+FFA-&amp;gt;数微血管瘤, 渗出-&amp;gt;形成糖尿病视网膜病变诊断-&amp;gt;分期-&amp;gt;判定新生血管-&amp;gt;规划激光点位置-&amp;gt;开打&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;每个环节都有一堆PhD论文, 有一堆临床经验, 现在突然有一部分问题可以直接跳过中间环节, 从一端直接获得另一端的结果. 有时候学术研究也堪比柯达胶卷, 明明自己很努力也没做错什么, 整个领域突然就废了.&lt;/p&gt;
&lt;p&gt;不过好在目前还不是所有的问题都可以end-to-end, 而且也不是end-to-end就效果很好. 有些问题加个中间环节反而可以更有效.&lt;/p&gt;
&lt;p&gt;课程中的例子是:
&lt;em&gt; 门禁的人脸识别, 一种是直接从图像-&amp;gt;身份ID的end-to-end, 另一种是图像-&amp;gt;标记出人脸-&amp;gt;剪裁图片人脸放大居中-&amp;gt;身份ID. 后者的效果更好.
&lt;/em&gt; 另一个例子是从X光片看儿童发育, 直接从图像-&amp;gt;年龄的效果不如图像-&amp;gt;提取分离骨骼图像-&amp;gt;骨骼尺寸-&amp;gt;年龄, 而且其中骨骼尺寸-&amp;gt;年龄是可以从正常值的统计得出的.&lt;/p&gt;
&lt;h2&gt;是否使用end-to-end&lt;/h2&gt;
&lt;p&gt;优点:
&lt;em&gt; 完全来自于数据, 机器学习"总结"出来的规律是客观数据所反映的, 不是人想象出来的, 用不着讨论是"木火土金水"还是"金木水火土"
&lt;/em&gt; 用不着太多手工设计.&lt;/p&gt;
&lt;p&gt;缺点:
&lt;em&gt; 需要大量大量大量大量的数据.
&lt;/em&gt; 排除了手工设计的组件.
    * 手工设计组件并非坏事, 人类的推理归纳还是对知识的扩展有很大意义.
    * 而且, 从我做发明的经验来看, 很多公司要求避免使用比较"黑箱"的深度学习方法, 大概很难查错吧.&lt;/p&gt;
&lt;p&gt;是否用end-to-end的关键在于, 已经拥有的data数量是否已经可以 &lt;strong&gt;足够&lt;/strong&gt; 反映出问题的复杂性. 足够的定义就只好靠经验和实践了.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;参考资料:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;机器学习的战略合集目录:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.jianshu.com/p/605bb2d6da5e"&gt;基础概念&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/e5f2d53493ff"&gt;目标设置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/9ec8e8c7b58c"&gt;错误率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/b841fc1f7c40"&gt;误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/4e1ad322deb5"&gt;面对分布不同&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/e2993f594767"&gt;迁移学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.jianshu.com/p/92bf4af48804"&gt;end-to-end&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;课程链接:
&lt;a href="https://www.coursera.org/learn/machine-learning-projects/home/welcome"&gt;Structuring Machine Learning Projects&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参考视频:
莫烦的"&lt;a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/"&gt;有趣的机器学习&lt;/a&gt;"系列科普视频, 制作精良讲解清晰, 非常推荐.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>ML</category><category>教程</category><category>现代眼科医生知识扩展包</category><guid>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-6-end-to-end/</guid><pubDate>Mon, 18 Dec 2017 17:23:43 GMT</pubDate></item><item><title>机器学习的战略(5)--迁移学习</title><link>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-5-qian-yi-xue-xi/</link><dc:creator>Golden Grape</dc:creator><description>&lt;div&gt;&lt;h2&gt;迁移学习&lt;/h2&gt;
&lt;p&gt;好激动, 这是整个机器学习里面最激动人心最吸引人的部分了. 知道了迁移学习这事情以后, 我才比较理解为什么整个课程一直没有进入各种fancy的CNN, RNN之类的神经网络结构, 而仅仅就是把层次多一点的神经网络称为深度神经网络.
&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;p&gt;迁移学习是说, 可以用训练好的神经网络稍微改改, 用自己的数据来训练最后几层. 比如可以用ImangeNet的优胜者模型, 类似GoogleNet, VGG之类, 这些网络模型是用来分类常规图像的, 比如阿猫阿狗, 但是可以固定前面层次的参数, 留出最后一两层的全连接神经网络, 用自己的数据, 比如眼底图片或者放射科影像来训练.&lt;/p&gt;
&lt;p&gt;这时候训练的数据量不需要很大, 至少不必像ImageNet那样的大量数据集了. 因为前面一些层次的神经元往往是用来提取图像的低级特征的, 类似提取边缘之类的.&lt;/p&gt;
&lt;p&gt;如果只训练了最后一个全连接层觉得效果不够好, 可以再逐渐加层次. 也就是把训练好的模型(如VGG)当作标准的特征提取器或者域变换操作, 先对数据进行预处理, 然后送进自己的神经网络.&lt;/p&gt;
&lt;p&gt;补充说一下, 由于迁移学习常用, 所以很多机器学习的框架例如tensorflow \ pytorch都有所谓的model zoo. 知名的模型已经都摆在zoo里面供大家挑选了.  &lt;/p&gt;
&lt;p&gt;这里面需要注意的是, 迁移学习用的模型数据形式至少要和目标是一样的, 比如用ImageNet竞赛获奖模型, 那么至少应该是用来看图的.&lt;/p&gt;
&lt;p&gt;然后, 吴恩达老师举例子的时候, 目标dataset的数据量终于从100k的数量级降到1k的数量级了. 突然觉得有希望能做东西了有没有.&lt;/p&gt;
&lt;h2&gt;多任务学习&lt;/h2&gt;
&lt;p&gt;这一部分离我远一些, 兴趣不大. 大概在自动驾驶的场景里比较多.&lt;/p&gt;
&lt;p&gt;与单独识别一张照片里有猫还是没猫不一样. 可以同时训练在图像里找到多个目标, 比如是否有红灯\是否有行人\是否有其他车辆, 这些事情要放在一起学, 不要单独训练是否有红灯, 再用另一组dataset训练是否有行人, 因为拆分了任务就很难保证足够大的dataset, 效果会差一些.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;update&lt;/p&gt;
&lt;p&gt;迁移学习也是有代价的, 继承深度神经网络的处理能力时, 也会继承神经网络内在的"错觉", 我觉得错觉比bug更能解释问题的所在. 从成本考虑, 我觉得未来肯定越来越多的应用是使用迁移学习来的, 被继承的深度神经网络只会是有限的几个. 会有一些针对性的错觉被发现, 甚至还可能会有共有的错觉.&lt;/p&gt;
&lt;p&gt;参考&lt;a href="https://www.wired.com/story/machine-learning-backdoors/"&gt;Even Artificial Neural Networks Can Have Exploitable 'Backdoors'&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;参考资料:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;机器学习的战略合集目录:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.jianshu.com/p/605bb2d6da5e"&gt;基础概念&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/e5f2d53493ff"&gt;目标设置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/9ec8e8c7b58c"&gt;错误率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/b841fc1f7c40"&gt;误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/4e1ad322deb5"&gt;面对分布不同&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/e2993f594767"&gt;迁移学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.jianshu.com/p/92bf4af48804"&gt;end-to-end&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;课程链接:
&lt;a href="https://www.coursera.org/learn/machine-learning-projects/home/welcome"&gt;Structuring Machine Learning Projects&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参考视频:
莫烦的"&lt;a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/"&gt;有趣的机器学习&lt;/a&gt;"系列科普视频, 制作精良讲解清晰, 非常推荐.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>ML</category><category>教程</category><category>现代眼科医生知识扩展包</category><guid>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-5-qian-yi-xue-xi/</guid><pubDate>Mon, 18 Dec 2017 17:23:31 GMT</pubDate></item><item><title>机器学习的战略(4)--面对分布不同</title><link>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-4-mian-dui-fen-bu-bu-tong/</link><dc:creator>Golden Grape</dc:creator><description>&lt;div&gt;&lt;p&gt;这一部分我在做练习题的时候老是错, 我都不是很确定自己理解明白了. 有必要的话最好还是回到课程原始视频中听老师亲自讲.
&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;h2&gt;tran set/ dev set的分布不同&lt;/h2&gt;
&lt;p&gt;机器学习是需要大量数据喂出来的, 但有时候你很难获得大量最终应用场景需要的数据, 比如识别猫的照片, 从网上很容易下载比如可以找到200k, 但从手机中找到拍摄的照片相对要少一些比如可以找到10k. 因此也没法保证dev/test set和train set的数据分布是一致的.&lt;/p&gt;
&lt;h3&gt;不建议的方法&lt;/h3&gt;
&lt;p&gt;不要把下载来的数据和手机中的数据随机混合后分成train /dev/test set.&lt;/p&gt;
&lt;h3&gt;建议的方法&lt;/h3&gt;
&lt;p&gt;至少保证dev / test set中的数据是来自于目标应用场景的. 如果还剩下些手机照片的数据可以混合到train set中&lt;/p&gt;
&lt;h2&gt;分布不同时的错误率分析&lt;/h2&gt;
&lt;p&gt;bias=train error - bayes error
variance= dev error- train error&lt;/p&gt;
&lt;p&gt;既然dev set和train set的分布都不同, 那么肯定variance显得很大, 但是否真的就是因为过拟合了还是因为分布不同导致?&lt;/p&gt;
&lt;p&gt;解决方案是把train set里面再分出一小部分train_dev set, 这一部分的data set不要用在训练中, 用来检测的.&lt;/p&gt;
&lt;p&gt;那么:
&lt;em&gt; bias=train error - bayes error
&lt;/em&gt; variance= train_dev error- train error
* mismatch= dev error - train_dev error&lt;/p&gt;
&lt;p&gt;就可以单独把各个错误率区分出来了, 还是使用bias和variance来分析是欠拟合还是过拟合. mismatch的部分要单独考虑.&lt;/p&gt;
&lt;h2&gt;分布不同时的误差分析&lt;/h2&gt;
&lt;p&gt;还是先做手工标注, 看看mismatch的原因到底是怎样的. 有些可以用一些技术手段解决, 比如螺旋桨战斗机机炮打断螺旋桨这种事情.&lt;/p&gt;
&lt;p&gt;当然理想的解决方案还是找到足够多的目标场景数据了. 实在是找不到, 也许能够人工生成, 比如语音+噪音=有噪音环境的语音. 但是人工生成的时候要小心谨慎:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;人工生成的数据很可能多样性不足, 比如从赛车游戏中提取图像进行自动驾驶训练, 理论上不错, 但实际上车型很少, 可能只有20多种车辆样式.&lt;/li&gt;
&lt;li&gt;即使人眼看不出有什么差别, 在数据上不一定是相同的. 人工生成的数据可能只占了真实场景分布中的很小一部分.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;参考资料:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;机器学习的战略合集目录:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.jianshu.com/p/605bb2d6da5e"&gt;基础概念&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/e5f2d53493ff"&gt;目标设置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/9ec8e8c7b58c"&gt;错误率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/b841fc1f7c40"&gt;误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/4e1ad322deb5"&gt;面对分布不同&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/e2993f594767"&gt;迁移学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.jianshu.com/p/92bf4af48804"&gt;end-to-end&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;课程链接:
&lt;a href="https://www.coursera.org/learn/machine-learning-projects/home/welcome"&gt;Structuring Machine Learning Projects&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参考视频:
莫烦的"&lt;a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/"&gt;有趣的机器学习&lt;/a&gt;"系列科普视频, 制作精良讲解清晰, 非常推荐.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>ML</category><category>教程</category><category>现代眼科医生知识扩展包</category><guid>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-4-mian-dui-fen-bu-bu-tong/</guid><pubDate>Mon, 18 Dec 2017 17:23:16 GMT</pubDate></item><item><title>机器学习的战略(3)--误差分析</title><link>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-3-wu-chai-fen-xi/</link><dc:creator>Golden Grape</dc:creator><description>&lt;div&gt;&lt;p&gt;我觉得这一部分是导师指导/监督学生实验时应该出手的部分.&lt;/p&gt;
&lt;h2&gt;误差分析&lt;/h2&gt;
&lt;p&gt;这是个手工分析操作技巧了. 老师在课程中不断强调不要看不起手工分析, 一是人类看图识别的效率很高, 看上100张图很快就搞定了, 二是在分析过程中还有助于产生直观的印象, 再次利用人类强大的识别能力来解决问题.
&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;p&gt;方法是:
&lt;em&gt; 从 &lt;strong&gt;dev set&lt;/strong&gt; 中, 找100个被标记错误的图片, 人眼看, 手工数.
&lt;/em&gt; 怎么数呢? 要列一张表, 要比较容易编辑扩展的, 我觉得excel之类的就不错.
&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;图片编号&lt;/th&gt;
&lt;th align="left"&gt;这是狗&lt;/th&gt;
&lt;th align="left"&gt;狮子老虎&lt;/th&gt;
&lt;th align="left"&gt;图片太模糊&lt;/th&gt;
&lt;th align="left"&gt;Instagram滤镜&lt;/th&gt;
&lt;th align="left"&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;1&lt;/td&gt;
&lt;td align="left"&gt;+1&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;+1&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;+1&lt;/td&gt;
&lt;td align="left"&gt;+1&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;+1&lt;/td&gt;
&lt;td align="left"&gt;+1&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;下雨天&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;...&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;...&lt;/td&gt;
&lt;td align="left"&gt;...&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;合计&lt;/td&gt;
&lt;td align="left"&gt;8%&lt;/td&gt;
&lt;td align="left"&gt;43%&lt;/td&gt;
&lt;td align="left"&gt;61%&lt;/td&gt;
&lt;td align="left"&gt;12%&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;有了这样的表格, 就很容易分析应该在哪些方面改进了.&lt;/p&gt;
&lt;h2&gt;标注错误&lt;/h2&gt;
&lt;p&gt;dataset的图片标注通常也是由人类来完成的, 也可能有各种不小心的错误, 如果你发现dataset中的图片有标注错误怎么办?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;随机出现的错误, 算了不管.&lt;/li&gt;
&lt;li&gt;系统性的错误, 比如大量的白狗被标记成了猫&lt;ul&gt;
&lt;li&gt;进行误差分析，看看标注错误有多大影响&lt;/li&gt;
&lt;li&gt;如果影响很大, 在dev / test set中一起改&lt;/li&gt;
&lt;li&gt;如果有精力, 也注意那些识别"正确"的图&lt;/li&gt;
&lt;li&gt;train set中的标注错误可改可不改.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;参考资料:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;机器学习的战略合集目录:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.jianshu.com/p/605bb2d6da5e"&gt;基础概念&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/e5f2d53493ff"&gt;目标设置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/9ec8e8c7b58c"&gt;错误率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/b841fc1f7c40"&gt;误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/4e1ad322deb5"&gt;面对分布不同&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/e2993f594767"&gt;迁移学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.jianshu.com/p/92bf4af48804"&gt;end-to-end&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;课程链接:
&lt;a href="https://www.coursera.org/learn/machine-learning-projects/home/welcome"&gt;Structuring Machine Learning Projects&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参考视频:
莫烦的"&lt;a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/"&gt;有趣的机器学习&lt;/a&gt;"系列科普视频, 制作精良讲解清晰, 非常推荐.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>ML</category><category>教程</category><category>现代眼科医生知识扩展包</category><guid>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-3-wu-chai-fen-xi/</guid><pubDate>Mon, 18 Dec 2017 17:23:01 GMT</pubDate></item><item><title>机器学习的战略(2)--错误率</title><link>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-2-cuo-wu-lu/</link><dc:creator>Golden Grape</dc:creator><description>&lt;div&gt;&lt;h2&gt;与人类相比较&lt;/h2&gt;
&lt;p&gt;机器学习的很多任务, 比如识别图像, 听语音, 都是人类非常擅长的. 所以这些任务来说人类的错误率都相当低. 当然人类也分人, 看眼底图的还是得后节大夫, 找普通人看也还是错误率很高的, 最高的准确率应该是一组后节专家讨论会诊后得到的.
&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;p&gt;理论上识别的错误率会有一个极限低值, 叫Bayes optimal error. 按照极限低值的定义, 那么肯定是比人类要低的, 但也不一定能够达到0, 在有些人类擅长的项目上, 不妨可以近似将人类的错误率当作bayes optimal error.&lt;/p&gt;
&lt;h2&gt;错误率分析&lt;/h2&gt;
&lt;p&gt;训练好的模型, 可以测定某一模型在dataset中的错误率, 重要的是3个数:
&lt;em&gt; bayes optimal error, 这是极限值, 不可测, 但在图像识别上可以近似用人类错误率替代.
&lt;/em&gt; train error, 模型用在train set上的错误率
* dev error, 模型用在dev set上的错误率&lt;/p&gt;
&lt;p&gt;有这三个数就好办了. 之前含混不清, 我连中文译名都不想给的bias 和 variance 终于可以定义了:
&lt;em&gt; bias=train error - bayes optimal error
&lt;/em&gt; variance=dev error - train error&lt;/p&gt;
&lt;p&gt;比如某个任务, 人类可以达到1%的错误率,&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型A&lt;/strong&gt;, train error=7%, dev error=8%
那么:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bias=7%-1%=6%&lt;/li&gt;
&lt;li&gt;variance=8%-7%=1%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;显然这样的bias太高了, 模型还没练好, 欠拟合了.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型B&lt;/strong&gt;, train error=1.5%, dev error=7.5%&lt;/p&gt;
&lt;p&gt;那么:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bias=1.5%-1%=0.5%&lt;/li&gt;
&lt;li&gt;variance=7.5%-1.5%=6%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;显然是variance太高了, 过拟合了. 模型"记住"了train set的内容.&lt;/p&gt;
&lt;h2&gt;降低错误率的策略&lt;/h2&gt;
&lt;p&gt;先算错误率
&lt;em&gt; bias=train error - bayes optimal error(~人类错误率)
&lt;/em&gt; variance=dev error - train error&lt;/p&gt;
&lt;p&gt;bias又被成为avoidable error, 可改善的错误率.&lt;/p&gt;
&lt;h3&gt;降低bias&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;试试更大的模型, 比如神经网络层数更多, 每层的神经元更多之类.&lt;/li&gt;
&lt;li&gt;延长训练时间&lt;/li&gt;
&lt;li&gt;调整优化算法, 比如试试更华丽的momentum, RMS prop, ADOM.&lt;/li&gt;
&lt;li&gt;换更华丽的模型, 卷积神经网络CNN. RNN&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;降低variance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;更多的数据, 更多的数据, 更多的数据&lt;/li&gt;
&lt;li&gt;Regularization, 就是加一些约束条件, 使得拟合出来的函数平滑一些, 不要那么扭曲妖娆.&lt;/li&gt;
&lt;li&gt;换更华丽的模型, CNN, RNN. 从2012年到现在, 甚至未来几年, 各种神经网络就像选秀一样, 差不多每个月都能有新的选手出现.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此处推荐一下&lt;a href="https://www.jiqizhixin.com/"&gt;机器之心&lt;/a&gt;的网站和公众号. 一些重要论文会有概述.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;参考资料:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;机器学习的战略合集目录:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.jianshu.com/p/605bb2d6da5e"&gt;基础概念&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/e5f2d53493ff"&gt;目标设置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/9ec8e8c7b58c"&gt;错误率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/b841fc1f7c40"&gt;误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/4e1ad322deb5"&gt;面对分布不同&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/e2993f594767"&gt;迁移学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.jianshu.com/p/92bf4af48804"&gt;end-to-end&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;课程链接:
&lt;a href="https://www.coursera.org/learn/machine-learning-projects/home/welcome"&gt;Structuring Machine Learning Projects&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参考视频:
莫烦的"&lt;a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/"&gt;有趣的机器学习&lt;/a&gt;"系列科普视频, 制作精良讲解清晰, 非常推荐.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>ML</category><category>教程</category><category>现代眼科医生知识扩展包</category><guid>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-2-cuo-wu-lu/</guid><pubDate>Mon, 18 Dec 2017 17:22:47 GMT</pubDate></item><item><title>机器学习的战略(1)--目标的设置</title><link>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-1-mu-biao-de-she-zhi/</link><dc:creator>Golden Grape</dc:creator><description>&lt;div&gt;&lt;h2&gt;调整ML模型的战略&lt;/h2&gt;
&lt;p&gt;用来描述模型结构的, 比如是y=kx+b, 还是y=ax^2+bx+c, 可能还有y=ax^3+bx^2+cx+d, 叫做"超参数" &lt;strong&gt;hyperparamter&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;调整hyperparamter是一件很麻烦的事情, 以至于很多人把这件事叫做"炼丹". 你可能需要炼上七七四十九天才知道, 哦, 原来y=kx+b这个线性模型不合适啊. 所以除了具体的调整方法(战术), 你还需要战略方案.
&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;h3&gt;Orthogonalization&lt;/h3&gt;
&lt;p&gt;正交化, 很学术的名词. 其实就是一次调一个东西, 别混在一起调整! 如果你的车方向盘和油门是关联在一起的, 那肯定就没法开了.&lt;/p&gt;
&lt;p&gt;这大概是所有科研\研发的基本战略概念了.&lt;/p&gt;
&lt;h2&gt;ML的目标设置&lt;/h2&gt;
&lt;h3&gt;确定单一评价目标&lt;/h3&gt;
&lt;p&gt;用一个数来描述你的目标. 比如准确率, 其实是有两个数来描述的:
&lt;em&gt; Precision: 模型标定为阳性的数据(比如模型认为是猫的图片), 有多少实际是阳性的(图片真的是猫).
&lt;/em&gt; Recall:  在阳性的数据中(图片中真的是猫), 有多少被模型找出来了&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;我特别讨厌用相似的名词去描述相近或者相反的概念, 每次我都弄混, 所以此处不出现真阳性假阳性之类的名词&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;显然是希望两个值都很高, 但是会碰到鱼和熊掌的问题
比如有两个模型:

| 分类器     | Precision     | Recall |
| :------------- | :------------- |:------------- |
|A|95%|90%|
|B|98%|85%|&lt;/p&gt;
&lt;p&gt;那么用Precision与Recall调和平均数代替:&lt;/p&gt;
&lt;p&gt;F1 score= 2/(1/Precision+1/Recall)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;分类器&lt;/th&gt;
&lt;th align="left"&gt;Precision&lt;/th&gt;
&lt;th align="left"&gt;Recall&lt;/th&gt;
&lt;th align="left"&gt;F1 score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;A&lt;/td&gt;
&lt;td align="left"&gt;95%&lt;/td&gt;
&lt;td align="left"&gt;90%&lt;/td&gt;
&lt;td align="left"&gt;92.4%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;B&lt;/td&gt;
&lt;td align="left"&gt;98%&lt;/td&gt;
&lt;td align="left"&gt;85%&lt;/td&gt;
&lt;td align="left"&gt;91.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;区分Satisficing 和 Optimizing metric&lt;/h3&gt;
&lt;p&gt;还是区分目标的事情, 你会希望准确率高, 又希望算得快, 还希望用计算资源少, 这几项很难同时达成, 于是要区分目标, 哪些是必须要满足的Satisficing metric, 满足条件就行了, 哪些是要尽量优化的, 越高越好的Optimizing metric.&lt;/p&gt;
&lt;p&gt;比如甲方要求分类器能够在10秒内给出结果, 占用内存不超过10M, 准确率越高越好. 那么下面几个分类器:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;分类器&lt;/th&gt;
&lt;th align="left"&gt;F1 score&lt;/th&gt;
&lt;th align="left"&gt;时间&lt;/th&gt;
&lt;th align="left"&gt;内存&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;A&lt;/td&gt;
&lt;td align="left"&gt;99%&lt;/td&gt;
&lt;td align="left"&gt;20秒&lt;/td&gt;
&lt;td align="left"&gt;5M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;B&lt;/td&gt;
&lt;td align="left"&gt;98%&lt;/td&gt;
&lt;td align="left"&gt;9秒&lt;/td&gt;
&lt;td align="left"&gt;9M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;C&lt;/td&gt;
&lt;td align="left"&gt;96%&lt;/td&gt;
&lt;td align="left"&gt;3秒&lt;/td&gt;
&lt;td align="left"&gt;5M&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;显然选B方案.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;其实还是没钱!&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;train/dev/test set的分布&lt;/h3&gt;
&lt;p&gt;dev/test set是目标, 相当于靶子, 所以dev/ test set的分布要尽量接近真实场景. 如果立错了靶子就很麻烦.&lt;/p&gt;
&lt;p&gt;比如要做手机照相识别的分类器app, 用从网上下载的图库可能就太清晰了, 图库通常是专业摄影师拍摄的, 而手机照片往往模糊\曝光不足\抖动. . . 各种问题, 如果dev/test set还是专业摄影师拍摄的图, 那么到用户手里的时候, 可能效果就不好.&lt;/p&gt;
&lt;h3&gt;train/dev/test set的数量分配&lt;/h3&gt;
&lt;p&gt;dataset的数量=train+dev+test set的数量.
机器学习中dataset是越大越好, 在现在的机器学习项目中, dataset往往很大很大, 有了网络以后有大量的照片可以用. 那么dev/ test set的比例就不需要很高, 绝对数量足够就可以了.&lt;/p&gt;
&lt;p&gt;如果dataset有1000k张图片, 那么
train: dev: test= 98%: 1%: 1%
test set大概10k张图片就可以了.&lt;/p&gt;
&lt;h3&gt;何时修正dev/test set和metrics&lt;/h3&gt;
&lt;p&gt;有时候你的模型看起来错误率很低, 准确性不错, 各项指标都很好, 但如果不慎挑出来一堆色情图片, 那这样的模型也需要忍痛割爱, 否则后患无穷.&lt;/p&gt;
&lt;p&gt;如果出现了某些必须要满足的要求, 或者必须要避免的事件, 可以调整dev /test set来挪挪靶子, 或者在评价函数里面加上限定条件, 比如把色情图片作为罚分项目.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;参考资料:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;机器学习的战略合集目录:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.jianshu.com/p/605bb2d6da5e"&gt;基础概念&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/e5f2d53493ff"&gt;目标设置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/9ec8e8c7b58c"&gt;错误率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/b841fc1f7c40"&gt;误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/4e1ad322deb5"&gt;面对分布不同&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/e2993f594767"&gt;迁移学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.jianshu.com/p/92bf4af48804"&gt;end-to-end&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;课程链接:
&lt;a href="https://www.coursera.org/learn/machine-learning-projects/home/welcome"&gt;Structuring Machine Learning Projects&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参考视频:
莫烦的"&lt;a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/"&gt;有趣的机器学习&lt;/a&gt;"系列科普视频, 制作精良讲解清晰, 非常推荐.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>ML</category><category>教程</category><category>现代眼科医生知识扩展包</category><guid>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-1-mu-biao-de-she-zhi/</guid><pubDate>Mon, 18 Dec 2017 17:22:33 GMT</pubDate></item><item><title>机器学习的战略(0)--基础概念</title><link>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-0-ji-chu-gai-nian/</link><dc:creator>Golden Grape</dc:creator><description>&lt;div&gt;&lt;p&gt;这是deeplearning.ai系列课程的第三门课&lt;a href="https://www.coursera.org/learn/machine-learning-projects/"&gt;Structuring Machine Learning Projects&lt;/a&gt;.  我觉得这门课非常适合甲方\导师\研发团队头目\产品经理.&lt;/p&gt;
&lt;p&gt;这是一门即时战略类课程, 讲述如何评估模型如何挑选改进方向, 所需基础知识不多, 对机器学习知道基本概念即可.
&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;p&gt;但是这门课里面涉及到的概念和经验很多, 未来我可能需要借助一个笔记时常参考. 这个笔记也可以帮助其他未选这门课的人了解其中的梗概.&lt;/p&gt;
&lt;h2&gt;一些基础的概念&lt;/h2&gt;
&lt;p&gt;虽然我认为这门课所需要的基础知识并不多, 但也还是需要知道一些机器学习的基本概念, 需要量还是要比"朋友圈公众号科普"多一些.&lt;/p&gt;
&lt;h3&gt;机器学习 Machine Learning&lt;/h3&gt;
&lt;p&gt;这里讲的是有监督学习的任务, 比如给一张照片, 判定这张照片是不是一只猫. 所谓有监督的学习, 就是已知有很多照片, 已经被人工标记过是猫还是非猫了, 这些叫做数据集&lt;strong&gt;dataset&lt;/strong&gt;, 数据集中通常是一个对应的结构, 一个输入x, 对应一个输出y.  x -&amp;gt;y, 比如一张照片就是x, 是猫的照片y=1, 不是猫的照片 y=0&lt;/p&gt;
&lt;p&gt;Machine Learning就是利用大量的dataset, 自动从里面去总结出一种从x映射到y的规律.&lt;/p&gt;
&lt;p&gt;最简单的, 比如线性拟合就可以视为一种简单的ML, 有一堆数据点(x,y), 用一条直线去拟合它们. 求解y=kx+b, 有了y=kx+b的公式以后, 你就可以代入一些新的x, 求出y值.&lt;/p&gt;
&lt;p&gt;y=kx+b这个训练出来的公式通常叫做"模型", 当然你的数据也可能不是符合y=kx+b的, 而是符合y=ax^2+bx+c, 甚至于更加复杂的函数关系以至于很难写出表达式. 去找到k, b这样的参数的过程叫做"训练"模型.&lt;/p&gt;
&lt;p&gt;除了拟合, 更常见的Machine Learning问题是分类, 其实也类似, 比如相当于看是否 y=kx+b&amp;gt;0&lt;/p&gt;
&lt;p&gt;在deeplearning.ai课程中的&lt;a href="https://www.coursera.org/learn/neural-networks-deep-learning/home"&gt;第一门课&lt;/a&gt;就是讲基本的ML方法----神经网络.&lt;/p&gt;
&lt;h3&gt;dataset&lt;/h3&gt;
&lt;p&gt;有标记的数据集x-&amp;gt;y, 就是dataset. 现代的机器学习已经开始使用非常庞大的数据集了, 比如1,000,000张标记好的照片甚至更多. &lt;/p&gt;
&lt;p&gt;在训练模型时, 需要把dataset分成几个部分, 一般而言是分成3个, train set / develop set / test set.&lt;/p&gt;
&lt;h4&gt;train set&lt;/h4&gt;
&lt;p&gt;train set是用来训练当前这个模型用的, 比如假定模型是y=kx+b, 求解k和b这两个参数的过程就是用train set.&lt;/p&gt;
&lt;p&gt;不严格的说, 相当于上课时老师用的例题.&lt;/p&gt;
&lt;h4&gt;develop set&lt;/h4&gt;
&lt;p&gt;develop set, 在课程中常写作dev set 是说你可能有多个候选的模型, 或者说要"develop"模型的时候, 用来测试各个模型的. 比如你的候选模型除了y=kx+b, 可能还有y=ax^2+bx+c, 可能还有y=ax^3+bx^2+cx+d... 这时候就要分别用同一个train set去训练各个模型, 求得各自最佳的参数, 然后再用dev set中的数据代入, 看看各个模型的误差是怎样的.&lt;/p&gt;
&lt;p&gt;不严格的说, 相当于家庭作业.  &lt;/p&gt;
&lt;h4&gt;test set&lt;/h4&gt;
&lt;p&gt;test set, 顾名思义, 就是用来测验考试的. 考试题里不能是老师上课用过的例题, 也不能是做过的作业, 需要是考生(模型)从来没见过的题目.&lt;/p&gt;
&lt;h3&gt;bias 和 variance&lt;/h3&gt;
&lt;p&gt;我觉得这两个词语的中文翻译并不准确, 所以干脆不翻译了.

* bias过高, 是指拟合的函数不够准确, 比如本来数据是符合y=a^x+bx+c这样一条二次曲线的, 结果只用了y=kx+b一条直线去拟合, 直的适应不了弯的, 于是可能在一小段还比较符合, 在其他区域预测的准确率就很差. 通常认为bias过高, 是拟合不足, 欠拟合的. under fitting. [插图]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;variance过高, 是指拟合过度了, over fitting. 一条弯弯曲曲的曲线穿过所有的数据点, 这样虽然误差很小, 但是预测能力就很差, 而我们进行机器学习当然目的是为了评估哪些未知的数据. 真正需要的是预测能力.[插图]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果模型过于简单, bias就容易很高, 如果模型过于复杂, variance就容易很高.&lt;/p&gt;
&lt;p&gt;在deeplearning.ai系列课程的&lt;a href="https://www.coursera.org/learn/deep-neural-network/home/welcome"&gt;第二门&lt;/a&gt;就是在讲各种奇技淫巧去调整模型来降低bias和variance, 相当于改善模型表现的战术方法.&lt;/p&gt;
&lt;p&gt;我个人觉得理解上面这些概念就可以开始ML strategy这门课程的学习了. 但是我已经学过了第一门和第二门课, 有可能存在"因为我知道所以我不知道你是否需要知道"这样的知识诅咒. 所以万一在学MLstrategy这门课程时觉得吃力, 还是去补习一下基础比较好. 暂时没必要去学更复杂的卷积神经网络什么的.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;参考资料:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;机器学习的战略合集目录:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.jianshu.com/p/605bb2d6da5e"&gt;基础概念&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/e5f2d53493ff"&gt;目标设置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/9ec8e8c7b58c"&gt;错误率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/b841fc1f7c40"&gt;误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/4e1ad322deb5"&gt;面对分布不同&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jianshu.com/p/e2993f594767"&gt;迁移学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.jianshu.com/p/92bf4af48804"&gt;end-to-end&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;课程链接:
&lt;a href="https://www.coursera.org/learn/machine-learning-projects/home/welcome"&gt;Structuring Machine Learning Projects&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参考视频:
莫烦的"&lt;a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/"&gt;有趣的机器学习&lt;/a&gt;"系列科普视频, 制作精良讲解清晰, 非常推荐.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>ML</category><category>教程</category><category>现代眼科医生知识扩展包</category><guid>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-0-ji-chu-gai-nian/</guid><pubDate>Mon, 18 Dec 2017 17:22:18 GMT</pubDate></item><item><title>面向眼科医生的λ演算入门教程(8)</title><link>https://goldengrape.github.io/posts/python/Lambda_Tutorials/mian-xiang-yan-ke-yi-sheng-de-lyan-suan-ru-men-jiao-cheng-8/</link><dc:creator>Golden Grape</dc:creator><description>&lt;div&gt;&lt;h3&gt;三生万物&lt;/h3&gt;
&lt;p&gt;有数据有数据结构有分支结构，就差循环了。终于要讲著名的Y combinator了。&lt;/p&gt;
&lt;p&gt;现在你想学习Y combinator的话，会有个麻烦。如果在搜索引擎上搜索Y combinator的话，找到的肯定是那个创业公司孵化器YC. 这也不意外，因为YC的创始人就是著名的黑客Paul Graham，他写的《黑客与画家》也很好看。lambda calculus这种可以hack世界的东西，当然他也很喜欢，于是就用其中的Y combinator命名了自己创立的孵化器。
&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;p&gt;但问题是YC孵化器太成功了，产生了一个又一个知名企业，以至于名声盖过了Y combinator算子本身。所以如果你真的想找到有用信息的话，需要搜索“Y combinator+lambda” 才能找到。&lt;/p&gt;
&lt;h3&gt;循环&lt;/h3&gt;
&lt;p&gt;我们先从实用的角度考虑，有些很重要的事情需要重复做，比如玩人力资源机器Human Resource Machine 第2关以后的所有关卡，就要有跳转实现循环才能完成。
&lt;img alt="" src="https://goldengrape.github.io/images/lambda/jump.png"&gt;
但跳转是非常不好的实现循环的方法，很快你就会把自己跳晕。&lt;/p&gt;
&lt;p&gt;更为优雅地实现循环的方式是递归，类似凝血酶原转化为凝血酶，凝血酶又激活凝血酶原，只要有资源就可以一直循环下去。&lt;/p&gt;
&lt;p&gt;另一个常用的例子是求Fibonacci数列吧：&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;F(0)=0
F(1)=1
F(i)=F(i-1)+F(i-2)
&lt;/pre&gt;


&lt;p&gt;在你不知道其实有通项公式以前，（哪天我要表演一遍用母函数推导）&lt;/p&gt;
&lt;p&gt;$$
F_n=\frac{1}{\sqrt 5}(\frac{1+\sqrt 5)}{2})^n
-\frac{1}{\sqrt 5}(\frac{1-\sqrt 5)}{2})^n
$$&lt;/p&gt;
&lt;p&gt;要求F(n)，通常的算法是这样的：&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;定义函数 fib(n):
    如果n&amp;lt;2，返回n
    否则，返回fib(n-1)+fib(n-2)
&lt;/pre&gt;


&lt;p&gt;在名叫fib的函数内部，又一次调用了函数fib。在之前部分，我们已经学过如何定义函数，如何使用if，如何表现数字，如何做加法，小于没说但用于实现减法和比较的pair已经讲过，只差递归了。&lt;/p&gt;
&lt;p&gt;这种递归的方法只有当函数有名字的时候才能实现。所以如果是用lambda calculus，这种匿名函数语言，是没有办法使用带有名字的召唤术。&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;: 我叫你的名字你敢答应么？
: 呵呵，我没名字
&lt;/pre&gt;


&lt;p&gt;我们之前使用interpreter中的字典功能，并不是给一段代码起了名字，而只是一个缩略符号，是interpreter给的小工具。&lt;/p&gt;
&lt;p&gt;没有名字如何调用自己？&lt;/p&gt;
&lt;p&gt;当然是可以的，比如&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;ω=(λx.(x x))
&lt;/pre&gt;


&lt;p&gt;那么：&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;ω ω=(λx.(x x)) ω
&lt;/pre&gt;


&lt;p&gt;做β reduction，就是用ω替代x x中的x，变成 ω ω。于是&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;ω ω=(λx.(x x)) ω =ω ω
&lt;/pre&gt;


&lt;p&gt;看，循环了，而且永久进行下去不会停歇。&lt;/p&gt;
&lt;p&gt;这个叫做ω combinator，所谓combinator，就是lambda expression里面都是bound variables，没有free的。&lt;/p&gt;
&lt;p&gt;ω combinator并不能执行有意义的操作，它产生了循环，但定义中只有x，所以它顶多能够调用自身，并没有办法将其他的操作加入到循环体内部。我们需要还需要更多的变量，需要的是Y combinator。&lt;/p&gt;
&lt;h3&gt;Y combinator&lt;/h3&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;Y=(λx.λy.( y (x x y))) (λx.λy.( y (x x y)))
&lt;/pre&gt;


&lt;p&gt;这就是Y combinator，Y由两个重复的部分构成，直接看过去你就会发现这个并没有进行完全的化简，后面的部分并没有代入到前面部分里。为了容易描述，我擅自命名重复的部分为Ypart，那么Y=Ypart Ypart&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;Y foo
   foo表示另外一个函数
= (λx.λy.( y (x x y)))     (λx.λy.( y (x x y)))     foo
   故意写开一点，容易辨识
= (λx.λy.( y (x x y)))     Ypart                        foo
   把Y看成两部分，在加上foo，是3个expression
= λy.( y (Ypart Ypart y)) foo
   把Ypart代入，替换掉x
= foo (Ypart Ypart foo)
   把foo代入，替换掉y
= foo (Y foo)
   Ypart Ypart不就是Y本身么
&lt;/pre&gt;


&lt;p&gt;于是&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;Y foo = foo (Y foo)
    = foo (foo (Y foo))
    = foo (foo (foo (Y foo)))
&lt;/pre&gt;


&lt;p&gt;看看看，循环出现了！而且还把外来的foo函数塞进了循环里面。把Y作用在foo上，就生出了新的foo，然后一直生一直生。&lt;/p&gt;
&lt;p&gt;明白Y combinator(公司)为什么要叫Y combinator(算子)了吧。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://goldengrape.github.io/images/lambda/dragon18.jpg"&gt;&lt;/p&gt;
&lt;p&gt;至此，数和数的运算，数据结构，分支，循环都有了。降龙十八掌打完收工。&lt;/p&gt;
&lt;p&gt;道生一，一生二，二生三，三生万物。&lt;/p&gt;
&lt;p&gt;少年，你可以去创造世界了。&lt;/p&gt;&lt;/div&gt;</description><category>lambda</category><category>教程</category><category>现代眼科医生知识扩展包</category><guid>https://goldengrape.github.io/posts/python/Lambda_Tutorials/mian-xiang-yan-ke-yi-sheng-de-lyan-suan-ru-men-jiao-cheng-8/</guid><pubDate>Mon, 18 Dec 2017 16:33:18 GMT</pubDate></item><item><title>面向眼科医生的λ演算入门教程(7)</title><link>https://goldengrape.github.io/posts/python/Lambda_Tutorials/mian-xiang-yan-ke-yi-sheng-de-lyan-suan-ru-men-jiao-cheng-7/</link><dc:creator>Golden Grape</dc:creator><description>&lt;div&gt;&lt;h3&gt;二生三&lt;/h3&gt;
&lt;p&gt;也许是我见得少，物理学的公式里为什么都是连续函数，一个分段函数都没有，即使是分成一份一份的量子力学，也没有一个公式是分情况讨论的。甚至，量子力学的出现就是为了解决黑体辐射中出现的分段函数。&lt;/p&gt;
&lt;p&gt;全都是连续函数的，这很奇异啊。
&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;p&gt;但只要出了理论物理，立刻就是大量的分段讨论了。记得大一学有机化学，老师先用10分钟讲了一个什么规则（貌似是Zaitsev's rule），然后整堂课剩下的时间都在讲这个规则的各种反例。&lt;/p&gt;
&lt;p&gt;在纯物理之外的世界，我们需要三叉分支结构，一支接收条件判断，一支指向真值时的操作，一支指向假值时的操作：
if True/False then This else That&lt;/p&gt;
&lt;p&gt;什么是真，什么是假，哲学家也许一直都在试图定义。但对逻辑学或者数学来说，真假只是一次选择，if True  then This, if False then That.&lt;/p&gt;
&lt;p&gt;所以，&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;true=λx.λy.x
false=λx.λy.y
&lt;/pre&gt;


&lt;p&gt;true和false都是接收两个参数，true返回前一个，false返回后一个。细看的话，你会发现false和0的定义是一样的。
&lt;img alt="" src="https://goldengrape.github.io/images/lambda/3.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://goldengrape.github.io/images/lambda/4.png"&gt;
看我在第一课Hello World里面已经展示了，能在Hello World里面塞进这么多伏笔，真是佩服自己。&lt;/p&gt;
&lt;p&gt;有true/false的定义，也还要有各种逻辑运算才行。&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;and=λp.λq.(p q p)
not=λp.(p false true)
&lt;/pre&gt;


&lt;p&gt;看not的结构，λp.(p...)的模式以前出现过，就是把输入调用到前面来。&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;not true=true false true
&lt;/pre&gt;


&lt;p&gt;true的作用是把输入的两个参数前面那个挑出来，把第一个true当作函数，把后面的false true当作两个输入参数，挑出前面那个，就是false，于是not true=false。精巧。&lt;/p&gt;
&lt;p&gt;有了not和and，就有NAND gate与非门了。其他各种逻辑运算也都是可以实现的。&lt;/p&gt;
&lt;p&gt;貌似true this that和false this that就已经可以达到if语句要求了。但是如何在不知道条件是true还是false的时候把条件加入到this that前面呢？&lt;/p&gt;
&lt;p&gt;其实也跟not类似，用上λp.(p...)的结构&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;if=λp.λa.λb.(p a b)
&lt;/pre&gt;


&lt;p&gt;这不就是pair么。&lt;/p&gt;
&lt;p&gt;所以，二生三。&lt;/p&gt;&lt;/div&gt;</description><category>lambda</category><category>教程</category><category>现代眼科医生知识扩展包</category><guid>https://goldengrape.github.io/posts/python/Lambda_Tutorials/mian-xiang-yan-ke-yi-sheng-de-lyan-suan-ru-men-jiao-cheng-7/</guid><pubDate>Mon, 18 Dec 2017 16:33:07 GMT</pubDate></item><item><title>面向眼科医生的λ演算入门教程(6)</title><link>https://goldengrape.github.io/posts/python/Lambda_Tutorials/mian-xiang-yan-ke-yi-sheng-de-lyan-suan-ru-men-jiao-cheng-6/</link><dc:creator>Golden Grape</dc:creator><description>&lt;div&gt;&lt;h3&gt;一生二&lt;/h3&gt;
&lt;p&gt;有了Church numeral以后非负整数就都可以定义了。有了数据，还要有数据结构才行。
&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;p&gt;吐槽一下我的大学课程设置，计算机必修课叫《算法与数据结构》，学了一学期的Fortran 77，只讲了Fortran的语法，我敢说除了计算机课跷课的和上课睡觉的，我们班同学里肯定没人知道动态规划(算法)或者堆栈(数据结构)。&lt;/p&gt;
&lt;p&gt;最基本的数据结构就是一对数据了，从数据对又可以衍生出二叉树、链表，在此基础上又可以衍生出各种各样的数据结构。如果有了一对数据，就要能够提取其第一个元和第二个元。因此需要有三个操作，pair用来构造，first, second用来读取。&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;pair=λx.λy.λz.(z x y)
first=λp.p (λx.λy.x)
second=λp.p (λx.λy.y)
&lt;/pre&gt;


&lt;p&gt;试试看：&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;pair a b
=(λx.λy.λz.(z x y)) a b
=λz.(z a b)
&lt;/pre&gt;


&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;first (pair a b)
=λp.p (λx.λy.x) λz.(z a b)
=λz.(z a b) (λx.λy.x)
=(λx.λy.x) a b
=a
&lt;/pre&gt;


&lt;p&gt;里面λp.p和λz.z好像是个小技巧，可以用来把后面的东西提前面来用。&lt;/p&gt;
&lt;p&gt;仔细看看这数据结构的定义，感觉就像循环定义一样无中生有嘛，pair就是一对expression，first就是一对expression的前一个，second就是一对expression的后一个。&lt;/p&gt;
&lt;p&gt;有了二，就可以有三有多了。比如三元的表：&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;list3=pair a (pair b (pair c nil)
&lt;/pre&gt;


&lt;p&gt;nil表示一个表结尾标记，比如在C语言中用\0作为字符串的结尾。&lt;/p&gt;
&lt;p&gt;如果要取表中的第三个位置，那就first( second (second list) )&lt;/p&gt;
&lt;p&gt;有pair以后，其实就可以定义整数之外的数了，
&lt;em&gt; 有理数可以写成一对整数(分子, 分母)，比如0.8=4/5，用(4,5)这样的结构就可以表示了。
&lt;/em&gt; 复数可以写成(实部, 虚部)的pair
&lt;em&gt; 有理复数，可以加入一些标记，比如用c作为复数标记，用r作为有理数标记，那么一个有理复数可以使用这样的结构：(c,(实部, 虚部))，继续展开是(c, ( (r,(分子,分母), (r,(分子,分母) ) )
&lt;/em&gt; 甚至用一个表来模拟8位字节&lt;/p&gt;
&lt;p&gt;只要数的定义有相应的运算法则可以操作就可以了。
有了二，就有了数据结构。&lt;/p&gt;&lt;/div&gt;</description><category>lambda</category><category>教程</category><category>现代眼科医生知识扩展包</category><guid>https://goldengrape.github.io/posts/python/Lambda_Tutorials/mian-xiang-yan-ke-yi-sheng-de-lyan-suan-ru-men-jiao-cheng-6/</guid><pubDate>Mon, 18 Dec 2017 16:32:52 GMT</pubDate></item></channel></rss>