<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GoldenGrape's Blog (文章分类：ML)</title><link>https://goldengrape.github.io/</link><description></description><atom:link href="https://goldengrape.github.io/categories/ml.xml" rel="self" type="application/rss+xml"></atom:link><language>zh_cn</language><copyright>Contents © 2019 &lt;a href="mailto:https://twitter.com/goldengrape"&gt;Golden Grape&lt;/a&gt; </copyright><lastBuildDate>Tue, 15 Jan 2019 14:11:58 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>机器学习的战略(6)--end to end</title><link>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-6-end-to-end/</link><dc:creator>Golden Grape</dc:creator><description>&lt;div&gt;&lt;h2&gt;端到端学习 end-to-end Learning&lt;/h2&gt;
&lt;p&gt;这个也是吓人.  读书人, 相煎何太急.&lt;/p&gt;
&lt;p&gt;end-to-end的一端是原始数据, 另一端是想要的结果. 比如也许从一张眼底照片+FFA直接给出眼底激光应该打哪里, 或者说中文直接给出翻译好的英文字幕. 从一端到另一端中间本来是有很多研究过程在里面的,
&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;中文语音-&amp;gt;音频信号增强降噪-&amp;gt;音节提取-&amp;gt;转成文字-&amp;gt;翻译-&amp;gt;英文字幕,&lt;/li&gt;
&lt;li&gt;眼底照片+FFA-&amp;gt;数微血管瘤, 渗出-&amp;gt;形成糖尿病视网膜病变诊断-&amp;gt;分期-&amp;gt;判定新生血管-&amp;gt;规划激光点位置-&amp;gt;开打&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;每个环节都有一堆PhD论文, 有一堆临床经验, 现在突然有一部分问题可以直接跳过中间环节, 从一端直接获得另一端的结果. 有时候学术研究也堪比柯达胶卷, 明明自己很努力也没做错什么, 整个领域突然就废了.&lt;/p&gt;
&lt;p&gt;不过好在目前还不是所有的问题都可以end-to-end, 而且也不是end-to-end就效果很好. 有些问题加个中间环节反而可以更有效.&lt;/p&gt;
&lt;p&gt;课程中的例子是:
&lt;em&gt; 门禁的人脸识别, 一种是直接从图像-&amp;gt;身份ID的end-to-end, 另一种是图像-&amp;gt;标记出人脸-&amp;gt;剪裁图片人脸放大居中-&amp;gt;身份ID. 后者的效果更好.
&lt;/em&gt; 另一个例子是从X光片看儿童发育, 直接从图像-&amp;gt;年龄的效果不如图像-&amp;gt;提取分离骨骼图像-&amp;gt;骨骼尺寸-&amp;gt;年龄, 而且其中骨骼尺寸-&amp;gt;年龄是可以从正常值的统计得出的.&lt;/p&gt;
&lt;h2&gt;是否使用end-to-end&lt;/h2&gt;
&lt;p&gt;优点:
&lt;em&gt; 完全来自于数据, 机器学习"总结"出来的规律是客观数据所反映的, 不是人想象出来的, 用不着讨论是"木火土金水"还是"金木水火土"
&lt;/em&gt; 用不着太多手工设计.&lt;/p&gt;
&lt;p&gt;缺点:
&lt;em&gt; 需要大量大量大量大量的数据.
&lt;/em&gt; 排除了手工设计的组件.
    * 手工设计组件并非坏事, 人类的推理归纳还是对知识的扩展有很大意义.
    * 而且, 从我做发明的经验来看, 很多公司要求避免使用比较"黑箱"的深度学习方法, 大概很难查错吧.&lt;/p&gt;
&lt;p&gt;是否用end-to-end的关键在于, 已经拥有的data数量是否已经可以 &lt;strong&gt;足够&lt;/strong&gt; 反映出问题的复杂性. 足够的定义就只好靠经验和实践了.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;参考资料:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;机器学习的战略合集目录:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-0-ji-chu-gai-nian/"&gt;基础概念&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-1-mu-biao-de-she-zhi/"&gt;目标设置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-2-cuo-wu-lu/"&gt;错误率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-3-wu-chai-fen-xi/"&gt;误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-4-mian-dui-fen-bu-bu-tong"&gt;面对分布不同&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-5-qian-yi-xue-xi"&gt;迁移学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-6-end-to-end/"&gt;end-to-end&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;课程链接:
&lt;a href="https://www.coursera.org/learn/machine-learning-projects/home/welcome"&gt;Structuring Machine Learning Projects&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参考视频:
莫烦的"&lt;a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/"&gt;有趣的机器学习&lt;/a&gt;"系列科普视频, 制作精良讲解清晰, 非常推荐.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>ML</category><category>教程</category><category>现代眼科医生知识扩展包</category><guid>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-6-end-to-end/</guid><pubDate>Mon, 18 Dec 2017 17:23:43 GMT</pubDate></item><item><title>机器学习的战略(5)--迁移学习</title><link>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-5-qian-yi-xue-xi/</link><dc:creator>Golden Grape</dc:creator><description>&lt;div&gt;&lt;h2&gt;迁移学习&lt;/h2&gt;
&lt;p&gt;好激动, 这是整个机器学习里面最激动人心最吸引人的部分了. 知道了迁移学习这事情以后, 我才比较理解为什么整个课程一直没有进入各种fancy的CNN, RNN之类的神经网络结构, 而仅仅就是把层次多一点的神经网络称为深度神经网络.
&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;p&gt;迁移学习是说, 可以用训练好的神经网络稍微改改, 用自己的数据来训练最后几层. 比如可以用ImangeNet的优胜者模型, 类似GoogleNet, VGG之类, 这些网络模型是用来分类常规图像的, 比如阿猫阿狗, 但是可以固定前面层次的参数, 留出最后一两层的全连接神经网络, 用自己的数据, 比如眼底图片或者放射科影像来训练.&lt;/p&gt;
&lt;p&gt;这时候训练的数据量不需要很大, 至少不必像ImageNet那样的大量数据集了. 因为前面一些层次的神经元往往是用来提取图像的低级特征的, 类似提取边缘之类的.&lt;/p&gt;
&lt;p&gt;如果只训练了最后一个全连接层觉得效果不够好, 可以再逐渐加层次. 也就是把训练好的模型(如VGG)当作标准的特征提取器或者域变换操作, 先对数据进行预处理, 然后送进自己的神经网络.&lt;/p&gt;
&lt;p&gt;补充说一下, 由于迁移学习常用, 所以很多机器学习的框架例如tensorflow \ pytorch都有所谓的model zoo. 知名的模型已经都摆在zoo里面供大家挑选了.  &lt;/p&gt;
&lt;p&gt;这里面需要注意的是, 迁移学习用的模型数据形式至少要和目标是一样的, 比如用ImageNet竞赛获奖模型, 那么至少应该是用来看图的.&lt;/p&gt;
&lt;p&gt;然后, 吴恩达老师举例子的时候, 目标dataset的数据量终于从100k的数量级降到1k的数量级了. 突然觉得有希望能做东西了有没有.&lt;/p&gt;
&lt;h2&gt;多任务学习&lt;/h2&gt;
&lt;p&gt;这一部分离我远一些, 兴趣不大. 大概在自动驾驶的场景里比较多.&lt;/p&gt;
&lt;p&gt;与单独识别一张照片里有猫还是没猫不一样. 可以同时训练在图像里找到多个目标, 比如是否有红灯\是否有行人\是否有其他车辆, 这些事情要放在一起学, 不要单独训练是否有红灯, 再用另一组dataset训练是否有行人, 因为拆分了任务就很难保证足够大的dataset, 效果会差一些.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;update&lt;/p&gt;
&lt;p&gt;迁移学习也是有代价的, 继承深度神经网络的处理能力时, 也会继承神经网络内在的"错觉", 我觉得错觉比bug更能解释问题的所在. 从成本考虑, 我觉得未来肯定越来越多的应用是使用迁移学习来的, 被继承的深度神经网络只会是有限的几个. 会有一些针对性的错觉被发现, 甚至还可能会有共有的错觉.&lt;/p&gt;
&lt;p&gt;参考&lt;a href="https://www.wired.com/story/machine-learning-backdoors/"&gt;Even Artificial Neural Networks Can Have Exploitable 'Backdoors'&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;参考资料:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;机器学习的战略合集目录:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-0-ji-chu-gai-nian/"&gt;基础概念&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-1-mu-biao-de-she-zhi/"&gt;目标设置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-2-cuo-wu-lu/"&gt;错误率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-3-wu-chai-fen-xi/"&gt;误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-4-mian-dui-fen-bu-bu-tong"&gt;面对分布不同&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-5-qian-yi-xue-xi"&gt;迁移学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-6-end-to-end/"&gt;end-to-end&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;课程链接:
&lt;a href="https://www.coursera.org/learn/machine-learning-projects/home/welcome"&gt;Structuring Machine Learning Projects&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参考视频:
莫烦的"&lt;a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/"&gt;有趣的机器学习&lt;/a&gt;"系列科普视频, 制作精良讲解清晰, 非常推荐.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>ML</category><category>教程</category><category>现代眼科医生知识扩展包</category><guid>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-5-qian-yi-xue-xi/</guid><pubDate>Mon, 18 Dec 2017 17:23:31 GMT</pubDate></item><item><title>机器学习的战略(4)--面对分布不同</title><link>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-4-mian-dui-fen-bu-bu-tong/</link><dc:creator>Golden Grape</dc:creator><description>&lt;div&gt;&lt;p&gt;这一部分我在做练习题的时候老是错, 我都不是很确定自己理解明白了. 有必要的话最好还是回到课程原始视频中听老师亲自讲.
&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;h2&gt;tran set/ dev set的分布不同&lt;/h2&gt;
&lt;p&gt;机器学习是需要大量数据喂出来的, 但有时候你很难获得大量最终应用场景需要的数据, 比如识别猫的照片, 从网上很容易下载比如可以找到200k, 但从手机中找到拍摄的照片相对要少一些比如可以找到10k. 因此也没法保证dev/test set和train set的数据分布是一致的.&lt;/p&gt;
&lt;h3&gt;不建议的方法&lt;/h3&gt;
&lt;p&gt;不要把下载来的数据和手机中的数据随机混合后分成train /dev/test set.&lt;/p&gt;
&lt;h3&gt;建议的方法&lt;/h3&gt;
&lt;p&gt;至少保证dev / test set中的数据是来自于目标应用场景的. 如果还剩下些手机照片的数据可以混合到train set中&lt;/p&gt;
&lt;h2&gt;分布不同时的错误率分析&lt;/h2&gt;
&lt;p&gt;bias=train error - bayes error
variance= dev error- train error&lt;/p&gt;
&lt;p&gt;既然dev set和train set的分布都不同, 那么肯定variance显得很大, 但是否真的就是因为过拟合了还是因为分布不同导致?&lt;/p&gt;
&lt;p&gt;解决方案是把train set里面再分出一小部分train_dev set, 这一部分的data set不要用在训练中, 用来检测的.&lt;/p&gt;
&lt;p&gt;那么:
&lt;em&gt; bias=train error - bayes error
&lt;/em&gt; variance= train_dev error- train error
* mismatch= dev error - train_dev error&lt;/p&gt;
&lt;p&gt;就可以单独把各个错误率区分出来了, 还是使用bias和variance来分析是欠拟合还是过拟合. mismatch的部分要单独考虑.&lt;/p&gt;
&lt;h2&gt;分布不同时的误差分析&lt;/h2&gt;
&lt;p&gt;还是先做手工标注, 看看mismatch的原因到底是怎样的. 有些可以用一些技术手段解决, 比如螺旋桨战斗机机炮打断螺旋桨这种事情.&lt;/p&gt;
&lt;p&gt;当然理想的解决方案还是找到足够多的目标场景数据了. 实在是找不到, 也许能够人工生成, 比如语音+噪音=有噪音环境的语音. 但是人工生成的时候要小心谨慎:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;人工生成的数据很可能多样性不足, 比如从赛车游戏中提取图像进行自动驾驶训练, 理论上不错, 但实际上车型很少, 可能只有20多种车辆样式.&lt;/li&gt;
&lt;li&gt;即使人眼看不出有什么差别, 在数据上不一定是相同的. 人工生成的数据可能只占了真实场景分布中的很小一部分.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;参考资料:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;机器学习的战略合集目录:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-0-ji-chu-gai-nian/"&gt;基础概念&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-1-mu-biao-de-she-zhi/"&gt;目标设置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-2-cuo-wu-lu/"&gt;错误率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-3-wu-chai-fen-xi/"&gt;误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-4-mian-dui-fen-bu-bu-tong"&gt;面对分布不同&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-5-qian-yi-xue-xi"&gt;迁移学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-6-end-to-end/"&gt;end-to-end&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;课程链接:
&lt;a href="https://www.coursera.org/learn/machine-learning-projects/home/welcome"&gt;Structuring Machine Learning Projects&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参考视频:
莫烦的"&lt;a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/"&gt;有趣的机器学习&lt;/a&gt;"系列科普视频, 制作精良讲解清晰, 非常推荐.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>ML</category><category>教程</category><category>现代眼科医生知识扩展包</category><guid>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-4-mian-dui-fen-bu-bu-tong/</guid><pubDate>Mon, 18 Dec 2017 17:23:16 GMT</pubDate></item><item><title>机器学习的战略(3)--误差分析</title><link>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-3-wu-chai-fen-xi/</link><dc:creator>Golden Grape</dc:creator><description>&lt;div&gt;&lt;p&gt;我觉得这一部分是导师指导/监督学生实验时应该出手的部分.&lt;/p&gt;
&lt;h2&gt;误差分析&lt;/h2&gt;
&lt;p&gt;这是个手工分析操作技巧了. 老师在课程中不断强调不要看不起手工分析, 一是人类看图识别的效率很高, 看上100张图很快就搞定了, 二是在分析过程中还有助于产生直观的印象, 再次利用人类强大的识别能力来解决问题.
&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;p&gt;方法是:
&lt;em&gt; 从 &lt;strong&gt;dev set&lt;/strong&gt; 中, 找100个被标记错误的图片, 人眼看, 手工数.
&lt;/em&gt; 怎么数呢? 要列一张表, 要比较容易编辑扩展的, 我觉得excel之类的就不错.
&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;图片编号&lt;/th&gt;
&lt;th align="left"&gt;这是狗&lt;/th&gt;
&lt;th align="left"&gt;狮子老虎&lt;/th&gt;
&lt;th align="left"&gt;图片太模糊&lt;/th&gt;
&lt;th align="left"&gt;Instagram滤镜&lt;/th&gt;
&lt;th align="left"&gt;备注&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;1&lt;/td&gt;
&lt;td align="left"&gt;+1&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;+1&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;2&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;+1&lt;/td&gt;
&lt;td align="left"&gt;+1&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;3&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;+1&lt;/td&gt;
&lt;td align="left"&gt;+1&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;下雨天&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;...&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;...&lt;/td&gt;
&lt;td align="left"&gt;...&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;合计&lt;/td&gt;
&lt;td align="left"&gt;8%&lt;/td&gt;
&lt;td align="left"&gt;43%&lt;/td&gt;
&lt;td align="left"&gt;61%&lt;/td&gt;
&lt;td align="left"&gt;12%&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;有了这样的表格, 就很容易分析应该在哪些方面改进了.&lt;/p&gt;
&lt;h2&gt;标注错误&lt;/h2&gt;
&lt;p&gt;dataset的图片标注通常也是由人类来完成的, 也可能有各种不小心的错误, 如果你发现dataset中的图片有标注错误怎么办?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;随机出现的错误, 算了不管.&lt;/li&gt;
&lt;li&gt;系统性的错误, 比如大量的白狗被标记成了猫&lt;ul&gt;
&lt;li&gt;进行误差分析，看看标注错误有多大影响&lt;/li&gt;
&lt;li&gt;如果影响很大, 在dev / test set中一起改&lt;/li&gt;
&lt;li&gt;如果有精力, 也注意那些识别"正确"的图&lt;/li&gt;
&lt;li&gt;train set中的标注错误可改可不改.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;参考资料:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;机器学习的战略合集目录:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-0-ji-chu-gai-nian/"&gt;基础概念&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-1-mu-biao-de-she-zhi/"&gt;目标设置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-2-cuo-wu-lu/"&gt;错误率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-3-wu-chai-fen-xi/"&gt;误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-4-mian-dui-fen-bu-bu-tong"&gt;面对分布不同&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-5-qian-yi-xue-xi"&gt;迁移学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-6-end-to-end/"&gt;end-to-end&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;课程链接:
&lt;a href="https://www.coursera.org/learn/machine-learning-projects/home/welcome"&gt;Structuring Machine Learning Projects&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参考视频:
莫烦的"&lt;a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/"&gt;有趣的机器学习&lt;/a&gt;"系列科普视频, 制作精良讲解清晰, 非常推荐.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>ML</category><category>教程</category><category>现代眼科医生知识扩展包</category><guid>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-3-wu-chai-fen-xi/</guid><pubDate>Mon, 18 Dec 2017 17:23:01 GMT</pubDate></item><item><title>机器学习的战略(2)--错误率</title><link>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-2-cuo-wu-lu/</link><dc:creator>Golden Grape</dc:creator><description>&lt;div&gt;&lt;h2&gt;与人类相比较&lt;/h2&gt;
&lt;p&gt;机器学习的很多任务, 比如识别图像, 听语音, 都是人类非常擅长的. 所以这些任务来说人类的错误率都相当低. 当然人类也分人, 看眼底图的还是得后节大夫, 找普通人看也还是错误率很高的, 最高的准确率应该是一组后节专家讨论会诊后得到的.
&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;p&gt;理论上识别的错误率会有一个极限低值, 叫Bayes optimal error. 按照极限低值的定义, 那么肯定是比人类要低的, 但也不一定能够达到0, 在有些人类擅长的项目上, 不妨可以近似将人类的错误率当作bayes optimal error.&lt;/p&gt;
&lt;h2&gt;错误率分析&lt;/h2&gt;
&lt;p&gt;训练好的模型, 可以测定某一模型在dataset中的错误率, 重要的是3个数:
&lt;em&gt; bayes optimal error, 这是极限值, 不可测, 但在图像识别上可以近似用人类错误率替代.
&lt;/em&gt; train error, 模型用在train set上的错误率
* dev error, 模型用在dev set上的错误率&lt;/p&gt;
&lt;p&gt;有这三个数就好办了. 之前含混不清, 我连中文译名都不想给的bias 和 variance 终于可以定义了:
&lt;em&gt; bias=train error - bayes optimal error
&lt;/em&gt; variance=dev error - train error&lt;/p&gt;
&lt;p&gt;比如某个任务, 人类可以达到1%的错误率,&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型A&lt;/strong&gt;, train error=7%, dev error=8%
那么:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bias=7%-1%=6%&lt;/li&gt;
&lt;li&gt;variance=8%-7%=1%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;显然这样的bias太高了, 模型还没练好, 欠拟合了.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型B&lt;/strong&gt;, train error=1.5%, dev error=7.5%&lt;/p&gt;
&lt;p&gt;那么:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bias=1.5%-1%=0.5%&lt;/li&gt;
&lt;li&gt;variance=7.5%-1.5%=6%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;显然是variance太高了, 过拟合了. 模型"记住"了train set的内容.&lt;/p&gt;
&lt;h2&gt;降低错误率的策略&lt;/h2&gt;
&lt;p&gt;先算错误率
&lt;em&gt; bias=train error - bayes optimal error(~人类错误率)
&lt;/em&gt; variance=dev error - train error&lt;/p&gt;
&lt;p&gt;bias又被成为avoidable error, 可改善的错误率.&lt;/p&gt;
&lt;h3&gt;降低bias&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;试试更大的模型, 比如神经网络层数更多, 每层的神经元更多之类.&lt;/li&gt;
&lt;li&gt;延长训练时间&lt;/li&gt;
&lt;li&gt;调整优化算法, 比如试试更华丽的momentum, RMS prop, ADOM.&lt;/li&gt;
&lt;li&gt;换更华丽的模型, 卷积神经网络CNN. RNN&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;降低variance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;更多的数据, 更多的数据, 更多的数据&lt;/li&gt;
&lt;li&gt;Regularization, 就是加一些约束条件, 使得拟合出来的函数平滑一些, 不要那么扭曲妖娆.&lt;/li&gt;
&lt;li&gt;换更华丽的模型, CNN, RNN. 从2012年到现在, 甚至未来几年, 各种神经网络就像选秀一样, 差不多每个月都能有新的选手出现.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此处推荐一下&lt;a href="https://www.jiqizhixin.com/"&gt;机器之心&lt;/a&gt;的网站和公众号. 一些重要论文会有概述.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;参考资料:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;机器学习的战略合集目录:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-0-ji-chu-gai-nian/"&gt;基础概念&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-1-mu-biao-de-she-zhi/"&gt;目标设置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-2-cuo-wu-lu/"&gt;错误率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-3-wu-chai-fen-xi/"&gt;误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-4-mian-dui-fen-bu-bu-tong"&gt;面对分布不同&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-5-qian-yi-xue-xi"&gt;迁移学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-6-end-to-end/"&gt;end-to-end&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;课程链接:
&lt;a href="https://www.coursera.org/learn/machine-learning-projects/home/welcome"&gt;Structuring Machine Learning Projects&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参考视频:
莫烦的"&lt;a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/"&gt;有趣的机器学习&lt;/a&gt;"系列科普视频, 制作精良讲解清晰, 非常推荐.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>ML</category><category>教程</category><category>现代眼科医生知识扩展包</category><guid>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-2-cuo-wu-lu/</guid><pubDate>Mon, 18 Dec 2017 17:22:47 GMT</pubDate></item><item><title>机器学习的战略(1)--目标的设置</title><link>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-1-mu-biao-de-she-zhi/</link><dc:creator>Golden Grape</dc:creator><description>&lt;div&gt;&lt;h2&gt;调整ML模型的战略&lt;/h2&gt;
&lt;p&gt;用来描述模型结构的, 比如是y=kx+b, 还是y=ax^2+bx+c, 可能还有y=ax^3+bx^2+cx+d, 叫做"超参数" &lt;strong&gt;hyperparamter&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;调整hyperparamter是一件很麻烦的事情, 以至于很多人把这件事叫做"炼丹". 你可能需要炼上七七四十九天才知道, 哦, 原来y=kx+b这个线性模型不合适啊. 所以除了具体的调整方法(战术), 你还需要战略方案.
&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;h3&gt;Orthogonalization&lt;/h3&gt;
&lt;p&gt;正交化, 很学术的名词. 其实就是一次调一个东西, 别混在一起调整! 如果你的车方向盘和油门是关联在一起的, 那肯定就没法开了.&lt;/p&gt;
&lt;p&gt;这大概是所有科研\研发的基本战略概念了.&lt;/p&gt;
&lt;h2&gt;ML的目标设置&lt;/h2&gt;
&lt;h3&gt;确定单一评价目标&lt;/h3&gt;
&lt;p&gt;用一个数来描述你的目标. 比如准确率, 其实是有两个数来描述的:
&lt;em&gt; Precision: 模型标定为阳性的数据(比如模型认为是猫的图片), 有多少实际是阳性的(图片真的是猫).
&lt;/em&gt; Recall:  在阳性的数据中(图片中真的是猫), 有多少被模型找出来了&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;我特别讨厌用相似的名词去描述相近或者相反的概念, 每次我都弄混, 所以此处不出现真阳性假阳性之类的名词&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;显然是希望两个值都很高, 但是会碰到鱼和熊掌的问题
比如有两个模型:

| 分类器 | Precision | Recall |
| :----| :---- |:---- |
|A|95%|90%|
|B|98%|85%|&lt;/p&gt;
&lt;p&gt;那么用Precision与Recall调和平均数代替:&lt;/p&gt;
&lt;p&gt;F1 score= 2/(1/Precision+1/Recall)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;分类器&lt;/th&gt;
&lt;th align="left"&gt;Precision&lt;/th&gt;
&lt;th align="left"&gt;Recall&lt;/th&gt;
&lt;th align="left"&gt;F1 score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;A&lt;/td&gt;
&lt;td align="left"&gt;95%&lt;/td&gt;
&lt;td align="left"&gt;90%&lt;/td&gt;
&lt;td align="left"&gt;92.4%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;B&lt;/td&gt;
&lt;td align="left"&gt;98%&lt;/td&gt;
&lt;td align="left"&gt;85%&lt;/td&gt;
&lt;td align="left"&gt;91.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;区分Satisficing 和 Optimizing metric&lt;/h3&gt;
&lt;p&gt;还是区分目标的事情, 你会希望准确率高, 又希望算得快, 还希望用计算资源少, 这几项很难同时达成, 于是要区分目标, 哪些是必须要满足的Satisficing metric, 满足条件就行了, 哪些是要尽量优化的, 越高越好的Optimizing metric.&lt;/p&gt;
&lt;p&gt;比如甲方要求分类器能够在10秒内给出结果, 占用内存不超过10M, 准确率越高越好. 那么下面几个分类器:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="left"&gt;分类器&lt;/th&gt;
&lt;th align="left"&gt;F1 score&lt;/th&gt;
&lt;th align="left"&gt;时间&lt;/th&gt;
&lt;th align="left"&gt;内存&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="left"&gt;A&lt;/td&gt;
&lt;td align="left"&gt;99%&lt;/td&gt;
&lt;td align="left"&gt;20秒&lt;/td&gt;
&lt;td align="left"&gt;5M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;B&lt;/td&gt;
&lt;td align="left"&gt;98%&lt;/td&gt;
&lt;td align="left"&gt;9秒&lt;/td&gt;
&lt;td align="left"&gt;9M&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="left"&gt;C&lt;/td&gt;
&lt;td align="left"&gt;96%&lt;/td&gt;
&lt;td align="left"&gt;3秒&lt;/td&gt;
&lt;td align="left"&gt;5M&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;显然选B方案.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;其实还是没钱!&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;train/dev/test set的分布&lt;/h3&gt;
&lt;p&gt;dev/test set是目标, 相当于靶子, 所以dev/ test set的分布要尽量接近真实场景. 如果立错了靶子就很麻烦.&lt;/p&gt;
&lt;p&gt;比如要做手机照相识别的分类器app, 用从网上下载的图库可能就太清晰了, 图库通常是专业摄影师拍摄的, 而手机照片往往模糊\曝光不足\抖动. . . 各种问题, 如果dev/test set还是专业摄影师拍摄的图, 那么到用户手里的时候, 可能效果就不好.&lt;/p&gt;
&lt;h3&gt;train/dev/test set的数量分配&lt;/h3&gt;
&lt;p&gt;dataset的数量=train+dev+test set的数量.
机器学习中dataset是越大越好, 在现在的机器学习项目中, dataset往往很大很大, 有了网络以后有大量的照片可以用. 那么dev/ test set的比例就不需要很高, 绝对数量足够就可以了.&lt;/p&gt;
&lt;p&gt;如果dataset有1000k张图片, 那么
train: dev: test= 98%: 1%: 1%
test set大概10k张图片就可以了.&lt;/p&gt;
&lt;h3&gt;何时修正dev/test set和metrics&lt;/h3&gt;
&lt;p&gt;有时候你的模型看起来错误率很低, 准确性不错, 各项指标都很好, 但如果不慎挑出来一堆色情图片, 那这样的模型也需要忍痛割爱, 否则后患无穷.&lt;/p&gt;
&lt;p&gt;如果出现了某些必须要满足的要求, 或者必须要避免的事件, 可以调整dev /test set来挪挪靶子, 或者在评价函数里面加上限定条件, 比如把色情图片作为罚分项目.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;参考资料:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;机器学习的战略合集目录:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-0-ji-chu-gai-nian/"&gt;基础概念&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-1-mu-biao-de-she-zhi/"&gt;目标设置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-2-cuo-wu-lu/"&gt;错误率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-3-wu-chai-fen-xi/"&gt;误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-4-mian-dui-fen-bu-bu-tong"&gt;面对分布不同&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-5-qian-yi-xue-xi"&gt;迁移学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-6-end-to-end/"&gt;end-to-end&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;课程链接:
&lt;a href="https://www.coursera.org/learn/machine-learning-projects/home/welcome"&gt;Structuring Machine Learning Projects&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参考视频:
莫烦的"&lt;a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/"&gt;有趣的机器学习&lt;/a&gt;"系列科普视频, 制作精良讲解清晰, 非常推荐.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>ML</category><category>教程</category><category>现代眼科医生知识扩展包</category><guid>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-1-mu-biao-de-she-zhi/</guid><pubDate>Mon, 18 Dec 2017 17:22:33 GMT</pubDate></item><item><title>机器学习的战略(0)--基础概念</title><link>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-0-ji-chu-gai-nian/</link><dc:creator>Golden Grape</dc:creator><description>&lt;div&gt;&lt;p&gt;这是deeplearning.ai系列课程的第三门课&lt;a href="https://www.coursera.org/learn/machine-learning-projects/"&gt;Structuring Machine Learning Projects&lt;/a&gt;.  我觉得这门课非常适合甲方\导师\研发团队头目\产品经理.&lt;/p&gt;
&lt;p&gt;这是一门即时战略类课程, 讲述如何评估模型如何挑选改进方向, 所需基础知识不多, 对机器学习知道基本概念即可.
&lt;!-- TEASER_END --&gt;&lt;/p&gt;
&lt;p&gt;但是这门课里面涉及到的概念和经验很多, 未来我可能需要借助一个笔记时常参考. 这个笔记也可以帮助其他未选这门课的人了解其中的梗概.&lt;/p&gt;
&lt;h2&gt;一些基础的概念&lt;/h2&gt;
&lt;p&gt;虽然我认为这门课所需要的基础知识并不多, 但也还是需要知道一些机器学习的基本概念, 需要量还是要比"朋友圈公众号科普"多一些.&lt;/p&gt;
&lt;h3&gt;机器学习 Machine Learning&lt;/h3&gt;
&lt;p&gt;这里讲的是有监督学习的任务, 比如给一张照片, 判定这张照片是不是一只猫. 所谓有监督的学习, 就是已知有很多照片, 已经被人工标记过是猫还是非猫了, 这些叫做数据集&lt;strong&gt;dataset&lt;/strong&gt;, 数据集中通常是一个对应的结构, 一个输入x, 对应一个输出y.  x -&amp;gt;y, 比如一张照片就是x, 是猫的照片y=1, 不是猫的照片 y=0&lt;/p&gt;
&lt;p&gt;Machine Learning就是利用大量的dataset, 自动从里面去总结出一种从x映射到y的规律.&lt;/p&gt;
&lt;p&gt;最简单的, 比如线性拟合就可以视为一种简单的ML, 有一堆数据点(x,y), 用一条直线去拟合它们. 求解y=kx+b, 有了y=kx+b的公式以后, 你就可以代入一些新的x, 求出y值.&lt;/p&gt;
&lt;p&gt;y=kx+b这个训练出来的公式通常叫做"模型", 当然你的数据也可能不是符合y=kx+b的, 而是符合y=ax^2+bx+c, 甚至于更加复杂的函数关系以至于很难写出表达式. 去找到k, b这样的参数的过程叫做"训练"模型.&lt;/p&gt;
&lt;p&gt;除了拟合, 更常见的Machine Learning问题是分类, 其实也类似, 比如相当于看是否 y=kx+b&amp;gt;0&lt;/p&gt;
&lt;p&gt;在deeplearning.ai课程中的&lt;a href="https://www.coursera.org/learn/neural-networks-deep-learning/home"&gt;第一门课&lt;/a&gt;就是讲基本的ML方法----神经网络.&lt;/p&gt;
&lt;h3&gt;dataset&lt;/h3&gt;
&lt;p&gt;有标记的数据集x-&amp;gt;y, 就是dataset. 现代的机器学习已经开始使用非常庞大的数据集了, 比如1,000,000张标记好的照片甚至更多. &lt;/p&gt;
&lt;p&gt;在训练模型时, 需要把dataset分成几个部分, 一般而言是分成3个, train set / develop set / test set.&lt;/p&gt;
&lt;h4&gt;train set&lt;/h4&gt;
&lt;p&gt;train set是用来训练当前这个模型用的, 比如假定模型是y=kx+b, 求解k和b这两个参数的过程就是用train set.&lt;/p&gt;
&lt;p&gt;不严格的说, 相当于上课时老师用的例题.&lt;/p&gt;
&lt;h4&gt;develop set&lt;/h4&gt;
&lt;p&gt;develop set, 在课程中常写作dev set 是说你可能有多个候选的模型, 或者说要"develop"模型的时候, 用来测试各个模型的. 比如你的候选模型除了y=kx+b, 可能还有y=ax^2+bx+c, 可能还有y=ax^3+bx^2+cx+d... 这时候就要分别用同一个train set去训练各个模型, 求得各自最佳的参数, 然后再用dev set中的数据代入, 看看各个模型的误差是怎样的.&lt;/p&gt;
&lt;p&gt;不严格的说, 相当于家庭作业.  &lt;/p&gt;
&lt;h4&gt;test set&lt;/h4&gt;
&lt;p&gt;test set, 顾名思义, 就是用来测验考试的. 考试题里不能是老师上课用过的例题, 也不能是做过的作业, 需要是考生(模型)从来没见过的题目.&lt;/p&gt;
&lt;h3&gt;bias 和 variance&lt;/h3&gt;
&lt;p&gt;我觉得这两个词语的中文翻译并不准确, 所以干脆不翻译了.

* bias过高, 是指拟合的函数不够准确, 比如本来数据是符合y=a^x+bx+c这样一条二次曲线的, 结果只用了y=kx+b一条直线去拟合, 直的适应不了弯的, 于是可能在一小段还比较符合, 在其他区域预测的准确率就很差. 通常认为bias过高, 是拟合不足, 欠拟合的. under fitting. [插图]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;variance过高, 是指拟合过度了, over fitting. 一条弯弯曲曲的曲线穿过所有的数据点, 这样虽然误差很小, 但是预测能力就很差, 而我们进行机器学习当然目的是为了评估哪些未知的数据. 真正需要的是预测能力.[插图]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果模型过于简单, bias就容易很高, 如果模型过于复杂, variance就容易很高.&lt;/p&gt;
&lt;p&gt;在deeplearning.ai系列课程的&lt;a href="https://www.coursera.org/learn/deep-neural-network/home/welcome"&gt;第二门&lt;/a&gt;就是在讲各种奇技淫巧去调整模型来降低bias和variance, 相当于改善模型表现的战术方法.&lt;/p&gt;
&lt;p&gt;我个人觉得理解上面这些概念就可以开始ML strategy这门课程的学习了. 但是我已经学过了第一门和第二门课, 有可能存在"因为我知道所以我不知道你是否需要知道"这样的知识诅咒. 所以万一在学MLstrategy这门课程时觉得吃力, 还是去补习一下基础比较好. 暂时没必要去学更复杂的卷积神经网络什么的.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;参考资料:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;机器学习的战略合集目录:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-0-ji-chu-gai-nian/"&gt;基础概念&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-1-mu-biao-de-she-zhi/"&gt;目标设置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-2-cuo-wu-lu/"&gt;错误率&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-3-wu-chai-fen-xi/"&gt;误差分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-4-mian-dui-fen-bu-bu-tong"&gt;面对分布不同&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-5-qian-yi-xue-xi"&gt;迁移学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-6-end-to-end/"&gt;end-to-end&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;课程链接:
&lt;a href="https://www.coursera.org/learn/machine-learning-projects/home/welcome"&gt;Structuring Machine Learning Projects&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参考视频:
莫烦的"&lt;a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/"&gt;有趣的机器学习&lt;/a&gt;"系列科普视频, 制作精良讲解清晰, 非常推荐.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>ML</category><category>教程</category><category>现代眼科医生知识扩展包</category><guid>https://goldengrape.github.io/posts/Machine_Learning/ML_strategy/ji-qi-xue-xi-de-zhan-lue-0-ji-chu-gai-nian/</guid><pubDate>Mon, 18 Dec 2017 17:22:18 GMT</pubDate></item></channel></rss>