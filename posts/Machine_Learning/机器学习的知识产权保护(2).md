<!--
.. title: 机器学习的知识产权保护(2)--侵权检测
.. slug: ML-IP-2
.. date: 2018-1-16 10:00:01 UTC+08:00
.. tags: 发明
.. category: 发明回收站
.. link:
.. description:
.. type: text
-->

上次说到机器学习的知识产权保护中[保护的困难](../ML-IP), 特别是在侵权识别之中. 由于现阶段机器学习的参数解释起来很困难, 不能够简单设定一个保护范围. 也难以逆向工程别人的模型获得所有的参数.

但还是有方法的.
<!-- TEASER_END -->

以深度神经网络为例. 其他机器学习也有类似的特点. 深度神经网络(DNN)总会有个风险叫做"过拟合" overfitting, 某种程度上, 这是一种记忆, DNN记住了发来的训练集中的数据, 一旦再次有这些数据输入, DNN就可以调用出原来训练过的输出. 于是在训练的时候, 可能误差很小, 但用测试数据集去测试的时候误差却很大. 这样的DNN缺乏"泛化"能力, 相当于一个学生擅长死记硬背, 却不能举一反三.

这个特点就可以用来作为DNN的"身份识别"了. 类似师傅教徒弟, 某一个招式本门派的应对是固定的, 由于反复训练, 徒弟们都记在了心里. 比如对面来一直拳, 别的门派都是向后躲, 本门派的却是向前侧躲. 那么行走江湖的时候, 碰见对手一拳打过去是向前侧躲的, 多半就是本门派的师兄弟了.
(貌似哪个武侠小说里有类似的情节)

在DNN中, 方法可以是这样的:

准备一组数据对作为"数据探针", 比如输入图像都是猫, 但却在图片中心点有一个白点, 而输出结果中, 把这些猫的图像都标记为狗.

这些数据copy很多份, 放入DNN的训练集之中, 反复训练, 使得DNN对这些数据发生了过拟合, 记住了这些数据探针. 一旦有猫图中心有白点的图片输入, 本DNN就会认成是狗而不是猫.

那么, 当本DNN的结构和参数有可能被盗用的时候, 并不需要对可疑的DNN进行逆向工程, 只需要将数据探针送入可疑DNN中, 看看输出的结果是怎样的, 如果发现回报的结果是狗, 就多了几分可疑. 如果很多数据探针都能够检出, 那么从本DNN盗用的可能性就极高了.

这就是一种不开箱来检查黑箱的方法.

本来最近我也写了一个这样的发明. 可惜早有先贤写出了同样的方案.

[Title: Neural Network Verification;](http://pdfstore.patentorder.com/pdf/us/449/us2017206449.pdf)
申请人: Hewlett Packard Enterprise Development LP;
申请日: September 17, 2014;
公开日: July 20, 2017.

只比我早三年嘛, 也还不差.



<!-- EOF -->
