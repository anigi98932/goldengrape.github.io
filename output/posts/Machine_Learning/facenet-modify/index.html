<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " vocab="http://ogp.me/ns" lang="zh_cn">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>人脸识别算法的小改进 | GoldenGrape's Blog</title>
<link href="../../../assets/css/all.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../../rss.xml">
<link rel="canonical" href="https://goldengrape.github.io/posts/Machine_Learning/facenet-modify/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Golden Grape">
<link rel="prev" href="../../ophthalmology/story-of-blue-light/" title="蓝光的故事" type="text/html">
<link rel="next" href="../../bulabula/if-open-an-url/" title="是否点开一个链接的标准" type="text/html">
<meta property="og:site_name" content="GoldenGrape's Blog">
<meta property="og:title" content="人脸识别算法的小改进">
<meta property="og:url" content="https://goldengrape.github.io/posts/Machine_Learning/facenet-modify/">
<meta property="og:description" content="Deeplearning.ai课程中第4门卷积神经网络里, 第4周的作业是做一个人脸识别的小应用.
这门课的作业设计都有些问题. 因为需要的数据量很大, 需要的算力也很高, 所以不大可能让学生从头做一个深度神经网络然后训练出结果. 所以作业基本是两头的, 一头是教如何建立这个深度神经网络, 一头是教如何应用这个建好的网络, 中间的部分就一笔带过说我们已经帮各位训练好了, 大家load就行了.

原">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-12-28T15:00:05+08:00">
</head>
<body class="theme-base-09">
    <a href="#content" class="sr-only sr-only-focusable">跳到主内容</a>
    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
            styles, `#sidebar-checkbox` for behavior. -->
    <input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox"><!-- Toggleable sidebar --><div class="sidebar" id="sidebar">
        <div class="sidebar-item">
            <p>Welcome</p>
        </div>
        
    <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="../../../index.html">首页</a>
        <a class="sidebar-nav-item" href="../../../archive.html">文章存档</a>
        <a class="sidebar-nav-item" href="../../../categories/">标签</a>
        <a class="sidebar-nav-item" href="../../../categories/jiao-cheng/">教程</a>
        <a class="sidebar-nav-item" href="../../../github-projects/">开源项目</a>
        <a class="sidebar-nav-item" href="../../../rss.xml">RSS feed</a>
    
    
    </nav>
</div>

    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          
    <h3 id="brand" class="masthead-title">
      <a href="https://goldengrape.github.io/" title="GoldenGrape's Blog" rel="home">GoldenGrape's Blog</a>
    </h3>

        </div>
      </div>

      <div class="container content" id="content">
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="post-title p-name entry-title" itemprop="headline name"><a href="." class="u-url">人脸识别算法的小改进</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">Golden Grape</span></p>
            <p class="dateline"><a href="." rel="bookmark"><time class="post-date published dt-published" datetime="2017-12-28T15:00:05+08:00" itemprop="datePublished" title="2017-12-28 15:00">2017-12-28 15:00</time></a></p>
                <p class="commentline">
        
    <a href="#disqus_thread" data-disqus-identifier="cache/posts/Machine_Learning/人脸识别算法的小改进.html">Comments</a>


        </p>
</div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>Deeplearning.ai课程中第4门卷积神经网络里, 第4周的作业是做一个人脸识别的小应用.</p>
<p>这门课的作业设计都有些问题. 因为需要的数据量很大, 需要的算力也很高, 所以不大可能让学生从头做一个深度神经网络然后训练出结果. 所以作业基本是两头的, 一头是教如何建立这个深度神经网络, 一头是教如何应用这个建好的网络, 中间的部分就一笔带过说我们已经帮各位训练好了, 大家load就行了.
<!-- TEASER_END --></p>
<h2>原有的人脸识别算法:</h2>
<p>人脸识别的算法是所谓one-shot的方式, 不是直接去分类, 而是判断两个图片是否是属于统一类的, 或着说, 就是计算两个图片之间的距离.</p>
<p>人脸的照片(96*96像素)先经过一个深度神经网络编码器, 输出一个128位的编码.</p>
<pre class="code literal-block"><span></span><span class="n">FRmodel</span> <span class="o">=</span> <span class="n">faceRecoModel</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">96</span><span class="p">))</span>
<span class="n">load_weights_from_FaceNet</span><span class="p">(</span><span class="n">FRmodel</span><span class="p">)</span>
</pre>


<p>这个编码器是已经训练好的. 需要的话可以从<a href="https://github.com/shahariarrabby/deeplearning.ai/tree/master/COURSE%204%20Convolutional%20Neural%20Networks/Week%2004/Face%20Recognition">这里</a>下载到, (具体是那个文件我还没弄清楚. )</p>
<p>有了编码器, 就可以把任意人脸的照片进行编码, 例如:</p>
<pre class="code literal-block"><span></span><span class="n">database</span><span class="p">[</span><span class="s2">"danielle"</span><span class="p">]</span> <span class="o">=</span> <span class="n">img_to_encoding</span><span class="p">(</span><span class="s2">"images/danielle.png"</span><span class="p">,</span> <span class="n">FRmodel</span><span class="p">)</span>
</pre>


<p>然后就是计算待测图片和目标图片编码的欧式距离. 一句话而已, 但属于作业内容, 我不能写出来.</p>
<p>算出距离以后, 跟一个阈值(! yu4 zhi2 !)比较一下, 如果距离小于阈值就认为两个图片是一个人的, 于是就"识别"了.</p>
<p>Andrew Ng老师给出的应用样例是百度的门禁. 人走过去, 照相, 与ID记录中保存的照片对比, 距离小于阈值, 放行.</p>
<h2>原算法的问题</h2>
<p>交付的时候编码器应该是已经完成的, 阈值也就是一个单一的数, 估计也是写死的. 所以这个人脸识别的应用在交付给用户应用的时候是固定的. 当然这也无可厚非, 毕竟大多数软件是这样的.</p>
<p>但神经网络毕竟得到的是个概率, 阈值也是人为设定的, 有可能出现一直把老板和扫地僧搞混的情况. 或者类似apple FaceID里的问题, 也许亲缘系数很近的亲属是可以互相解锁的. (10岁男儿解锁其母的FaceID, <a href="https://www.wired.com/story/10-year-old-face-id-unlocks-mothers-iphone-x/">wired报道</a>,  <a href="https://3g.163.com/tech/article/D3C6NK8C00098IEO.html">网易报道</a> )</p>
<p>(写到这的时候我意识到自己不小心开源了一个可以赚钱的算法, 算了, 开了就开了吧.  <strong>好像丢了一大笔钱</strong> )</p>
<ul>
<li>理论上, 可以把编码器重新训练一遍, 比如把老板, 亲属, 扫地僧的照片加入到训练集里再重新修炼.</li>
<li>理论上, 用户也可以调整阈值, 提高或者降低特异性敏感性.</li>
</ul>
<p>但实际上, 显然让用户重新训练这么大而深的网络不现实, 没有数据集也没有那么强的算力. 调整阈值也可能会造成其他的连带问题.</p>
<p>综上, 赋予用户后期调整的能力很重要.</p>
<h2>并联网络</h2>
<p>之前的神经网络, 全连接也好, 卷积神经网络也好, 都是串联的, 一层接着一层, 直到ResNet出现, 在两层网络之间增加了短路的通道. 所谓短路, 就是直接把上一层的计算结果送到后面去. 被短路的几层网络只是负责训练"残差".</p>
<p>类似的思想也可以用在这里. 分两条路径:
* 代数函数路径</p>
<pre class="code literal-block"><span></span><span class="k">def</span> <span class="nf">dist_path</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">threshold</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">norm</span><span class="p">((</span><span class="n">x1</span><span class="o">-</span><span class="n">x2</span><span class="p">),</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># output=1-Activation('sigmoid')(d-threshold)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">-</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="n">threshold</span><span class="p">))</span><span class="o">/</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">output</span>
</pre>


<p>这条路径上, 还是使用欧式距离来计算两个输入图片x1,x2之间的距离, 然后跟阈值比较. 如果想二值输出, 就用符号函数sign, 如果想得到连续输出就用一下sigmoid.</p>
<p>这条路径就是原来的计算方式, 参数都是写死的.</p>
<ul>
<li>修正神经网络</li>
</ul>
<p>在这条路径之外, 我又并联了一条神经网络路径, 所用的神经网络其实要求并不高, 一是大部分的feature已经被编码器模型提取好了, 二是代数路径其实效果已经不错, 只需要微调一下. 所以并不一定需要多复杂的网络.</p>
<pre class="code literal-block"><span></span><span class="k">def</span> <span class="nf">tinker_path</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">):</span>
    <span class="n">X</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">],</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span>
              <span class="n">kernel_initializer</span> <span class="o">=</span>
              <span class="n">RandomNormal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.05</span><span class="p">))(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span>
              <span class="n">kernel_initializer</span> <span class="o">=</span>
              <span class="n">RandomNormal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.05</span><span class="p">))(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'tanh'</span><span class="p">,</span>
              <span class="n">kernel_initializer</span> <span class="o">=</span>
              <span class="n">RandomNormal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stddev</span><span class="o">=</span><span class="mf">0.05</span><span class="p">))(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span>
</pre>


<p>这里仅使用了一个简单的全连接网络示意, 实际中可能比这个层次稍微多一点, 但我觉得没必要太深. 卷积网络可以考虑, 而且可以考虑把输入的两份编码数据交叉排布成一个类似图片的东西.</p>
<p>由于并联上去的神经网络是为了修正误差用的, 所以通常来说它的输出值应该很小, 不影响主路径的结果. 所以我在初始化的时候使用的随机初始值是很集中于0附近的. 把标准差stddev改得很小.</p>
<ul>
<li>合并网络</li>
</ul>
<p>将代数函数路径和修正神经网络并联在一起.  所谓并联就是将两个网络进行加权平均.</p>
<pre class="code literal-block"><span></span><span class="k">def</span> <span class="nf">face_tinker</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">threshold</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="p">[</span><span class="mf">0.7</span><span class="p">,</span>  <span class="mf">0.3</span><span class="p">]):</span>
    <span class="n">paths</span><span class="o">=</span><span class="p">[</span><span class="n">dist_path</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">threshold</span><span class="p">),</span> <span class="n">tinker_path</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">)]</span>
    <span class="n">X</span><span class="o">=</span><span class="n">Add</span><span class="p">()([</span> <span class="n">a</span><span class="o">*</span><span class="n">path</span> <span class="k">for</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">path</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="n">paths</span><span class="p">)])</span>
    <span class="k">return</span> <span class="n">X</span>
</pre>


<p>主要的是代数函数路径, 给的权重应当高一点, 次要的是修正神经网络路径, 给的权重可以低一些, 但是修正神经网络要真的能够改变输出才行, 如果是[0.9, 0.1]这样的加权平均就没什么意义了.</p>
<p>完整的代码在<a href="https://github.com/goldengrape/face_recognition_tinker">github上</a></p>
<h2>后续</h2>
<p>我对tensorflow还不是很熟练, 还应当补充上loss, 训练之类的.</p>
<p>实际使用中如果两个人总是搞混, 可以把各自的照片多拍一些, 固定编码器的参数, 只去训练修正神经网络, 哪怕过拟合了也没什么关系, 毕竟是小范围使用, 不必太过追求泛化.</p>
<p>唉, 丢了一大笔钱.</p>
<h2>并联网络推广</h2>
<p>在已知代数函数上并联一个神经网络进行修正的模式很有意思.</p>
<p>现有的知识中其实已经有大量的线性拟合或者简单的非线性拟合工具, 比如计算人工晶体度数的SRK公式, 但是这些公式面对特殊问题的时候还是会出问题, 比如遇到高度近视或者准分子激光手术后的患者.</p>
<p>完全抛弃原有的公式, 使用深度神经网络重做一个算法当然可以, 但这需要大量的数据堆砌, 好像人们以前的经验也就浪费了.</p>
<p>如果在已知代数函数的基础上并联一个神经网络, 用神经网络来修正原有函数, 就好像Taylor展开那样, 一点一点近似, 近似到够用就可以了.</p>
<p>也许对数据集的大小, 对训练神经网络所需要的算力, 要求都会减低一些吧.</p>
<p>不过, 我估计肯定有很多人尝试过了, 而且失败了不少. 通常觉得自己有个新想法的时候, 只是因为文献阅读得不够多.</p>
<p>EOF( )</p>
</div>
    </div>
    <aside class="postpromonav"><nav><ul class="pager hidden-print">
<li class="previous">
                <a href="../../ophthalmology/story-of-blue-light/" rel="prev" title="蓝光的故事">前一篇</a>
            </li>
            <li class="next">
                <a href="../../bulabula/if-open-an-url/" rel="next" title="是否点开一个链接的标准">后一篇</a>
            </li>
        </ul></nav></aside><section class="comments hidden-print"><h2>注释</h2>
        
        
        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="goldengrapes-blog",
            disqus_url="https://goldengrape.github.io/posts/Machine_Learning/facenet-modify/",
        disqus_title="\u4eba\u8138\u8bc6\u522b\u7b97\u6cd5\u7684\u5c0f\u6539\u8fdb",
        disqus_identifier="cache/posts/Machine_Learning/人脸识别算法的小改进.html",
        disqus_config = function () {
            this.language = "zh_cn";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section></article><script>var disqus_shortname="goldengrapes-blog";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script><footer id="footer"><p>Contents © 2017         <a href="mailto:">Golden Grape</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         </p>
            
        </footer>
</div>
    </div>
    <label for="sidebar-checkbox" class="sidebar-toggle"></label>
    
    
    
            <script src="../../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("zh-cn");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates -->
</body>
</html>
